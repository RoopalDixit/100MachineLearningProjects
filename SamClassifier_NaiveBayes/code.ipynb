{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88cce70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/roopaldixit/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/roopaldixit/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/roopaldixit/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/roopaldixit/Library/Python/3.9/lib/python/site-packages (from nltk) (2025.7.31)\n",
      "Requirement already satisfied: tqdm in /Users/roopaldixit/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef5ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources loaded successfully.\n",
      "Accuracy: 0.5\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.50      1.00      0.67         1\n",
      "        Spam       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "New email prediction: Ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopaldixit/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/roopaldixit/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/roopaldixit/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Set NLTK data path\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download NLTK resources with error handling\n",
    "try:\n",
    "    nltk.download('punkt', download_dir=nltk_data_path, quiet=True)\n",
    "    nltk.download('punkt_tab', download_dir=nltk_data_path, quiet=True)  # Added punkt_tab\n",
    "    nltk.download('stopwords', download_dir=nltk_data_path, quiet=True)\n",
    "    # Verify resources\n",
    "    word_tokenize(\"test sentence\")\n",
    "    stopwords.words('english')\n",
    "    print(\"NLTK resources loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load NLTK resources: {e}\")\n",
    "    print(\"Please ensure internet connectivity and write permissions for ~/nltk_data\")\n",
    "    print(\"Try running: nltk.download('punkt_tab', download_dir='~/nltk_data')\")\n",
    "    exit(1)\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'email': [\n",
    "        'Win a free iPhone now!!! Click here!',\n",
    "        'Meeting at 10am tomorrow, please confirm.',\n",
    "        'Get rich quick! Buy our course!',\n",
    "        'Lunch plans this weekend? Let me know.',\n",
    "        'Limited time offer! Discount viagra pills.',\n",
    "        'Project deadline is next Friday.'\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'ham', 'spam', 'ham']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing error: {e}\")\n",
    "        return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_email'] = df['email'].apply(preprocess_text)\n",
    "\n",
    "# Convert to features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_email'])\n",
    "y = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Predict on new email\n",
    "new_email = \"Win a free vacation! Click now!\"\n",
    "cleaned_new_email = preprocess_text(new_email)\n",
    "new_email_vector = vectorizer.transform([cleaned_new_email])\n",
    "prediction = nb_classifier.predict(new_email_vector)\n",
    "print(\"New email prediction:\", \"Spam\" if prediction[0] == 1 else \"Ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69eda2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SpamAssassin dataset...\n",
      "Extracting 20030228_spam.tar.bz2...\n",
      "Extracting 20030228_easy_ham.tar.bz2...\n",
      "Extracting 20030228_spam_2.tar.bz2...\n",
      "Extracting 20021010_spam.tar.bz2...\n",
      "Extracting 20021010_easy_ham.tar.bz2...\n",
      "Extracting 20021010_hard_ham.tar.bz2...\n",
      "Dataset extracted to spamassassin with spam/ham subdirectories.\n",
      "Loaded 7702 emails (2400 spam, 5302 ham)\n",
      "Preprocessing emails...\n",
      "Training Naive Bayes classifier...\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.9689\n",
      "Precision: 0.9337\n",
      "Recall: 0.9688\n",
      "F1 Score: 0.9509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1028   33]\n",
      " [  15  465]]\n",
      "\n",
      "Sample Email Classification:\n",
      "Spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Function to download and extract SpamAssassin dataset\n",
    "def download_spamassassin_dataset():\n",
    "    base_url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    dataset_files = [\n",
    "        \"20030228_spam.tar.bz2\",\n",
    "        \"20030228_easy_ham.tar.bz2\",\n",
    "        \"20030228_spam_2.tar.bz2\",\n",
    "        \"20021010_spam.tar.bz2\",\n",
    "        \"20021010_easy_ham.tar.bz2\",\n",
    "        \"20021010_hard_ham.tar.bz2\"\n",
    "    ]\n",
    "    extract_path = \"spamassassin\"\n",
    "    \n",
    "    if not os.path.exists(extract_path):\n",
    "        os.makedirs(extract_path)\n",
    "        print(\"Downloading SpamAssassin dataset...\")\n",
    "        for dataset_file in dataset_files:\n",
    "            dataset_path = dataset_file\n",
    "            try:\n",
    "                urllib.request.urlretrieve(base_url + dataset_file, dataset_path)\n",
    "                print(f\"Extracting {dataset_file}...\")\n",
    "                with tarfile.open(dataset_path, \"r:bz2\") as tar:\n",
    "                    tar.extractall(path=extract_path)\n",
    "                os.remove(dataset_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download/extract {dataset_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Organize into spam/ham directories\n",
    "        spam_dir = os.path.join(extract_path, 'spam')\n",
    "        ham_dir = os.path.join(extract_path, 'ham')\n",
    "        os.makedirs(spam_dir, exist_ok=True)\n",
    "        os.makedirs(ham_dir, exist_ok=True)\n",
    "        \n",
    "        for subdir in os.listdir(extract_path):\n",
    "            subdir_path = os.path.join(extract_path, subdir)\n",
    "            if os.path.isdir(subdir_path) and subdir not in ['spam', 'ham']:\n",
    "                for filename in os.listdir(subdir_path):\n",
    "                    src = os.path.join(subdir_path, filename)\n",
    "                    if 'spam' in subdir.lower():\n",
    "                        dst = os.path.join(spam_dir, f\"{subdir}_{filename}\")\n",
    "                    else:\n",
    "                        dst = os.path.join(ham_dir, f\"{subdir}_{filename}\")\n",
    "                    shutil.move(src, dst)\n",
    "                os.rmdir(subdir_path)\n",
    "        \n",
    "        print(f\"Dataset extracted to {extract_path} with spam/ham subdirectories.\")\n",
    "    return extract_path\n",
    "\n",
    "# Function to load emails from directory\n",
    "def load_emails(directory):\n",
    "    emails = []\n",
    "    labels = []\n",
    "    \n",
    "    for folder in ['ham', 'spam']:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise ValueError(f\"Directory {folder_path} not found. Check extraction.\")\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) and not filename.startswith('.'):\n",
    "                with open(file_path, 'r', encoding='latin-1', errors='ignore') as file:\n",
    "                    try:\n",
    "                        content = file.read()\n",
    "                        # Extract body (skip headers)\n",
    "                        body_start = content.find('\\n\\n') + 2 if '\\n\\n' in content else 0\n",
    "                        body = content[body_start:].strip()\n",
    "                        if body:  # Only append non-empty emails\n",
    "                            emails.append(body)\n",
    "                            labels.append(1 if folder == 'spam' else 0)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping {filename}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "    if not emails:\n",
    "        raise ValueError(\"No emails loaded. Check dataset extraction.\")\n",
    "    \n",
    "    print(f\"Loaded {len(emails)} emails ({sum(labels)} spam, {len(labels) - sum(labels)} ham)\")\n",
    "    return emails, labels\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and short tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Download and load dataset\n",
    "    dataset_path = download_spamassassin_dataset()\n",
    "    emails, labels = load_emails(dataset_path)\n",
    "    \n",
    "    # Preprocess emails\n",
    "    print(\"Preprocessing emails...\")\n",
    "    processed_emails = [preprocess_text(email) for email in emails if email.strip()]\n",
    "    \n",
    "    # Convert text to TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, min_df=2)\n",
    "    X = vectorizer.fit_transform(processed_emails)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    print(\"Training Naive Bayes classifier...\")\n",
    "    nb_classifier = MultinomialNB(alpha=0.5)\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Example of classifying a new email\n",
    "    sample_email = \"\"\"\n",
    "    Subject: Get Rich Quick!\n",
    "    Dear Friend, congratulations! You've won a million dollars! Click here to claim your prize now!\n",
    "    \"\"\"\n",
    "    processed_sample = preprocess_text(sample_email)\n",
    "    sample_vector = vectorizer.transform([processed_sample])\n",
    "    prediction = nb_classifier.predict(sample_vector)\n",
    "    print(\"\\nSample Email Classification:\")\n",
    "    print(\"Spam\" if prediction[0] == 1 else \"Not Spam\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
