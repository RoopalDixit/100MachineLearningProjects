{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the chatbot...\n",
      "Epoch: 01 | Loss: 4.873 | Teacher Forcing: 0.70\n",
      "Epoch: 02 | Loss: 4.446 | Teacher Forcing: 0.69\n",
      "Epoch: 03 | Loss: 4.102 | Teacher Forcing: 0.67\n",
      "Epoch: 04 | Loss: 3.896 | Teacher Forcing: 0.66\n",
      "Epoch: 05 | Loss: 3.667 | Teacher Forcing: 0.65\n",
      "Epoch: 06 | Loss: 3.598 | Teacher Forcing: 0.63\n",
      "Epoch: 07 | Loss: 3.424 | Teacher Forcing: 0.62\n",
      "Epoch: 08 | Loss: 3.203 | Teacher Forcing: 0.61\n",
      "Epoch: 09 | Loss: 3.023 | Teacher Forcing: 0.59\n",
      "Epoch: 10 | Loss: 2.668 | Teacher Forcing: 0.58\n",
      "Epoch: 11 | Loss: 2.553 | Teacher Forcing: 0.57\n",
      "Epoch: 12 | Loss: 2.379 | Teacher Forcing: 0.55\n",
      "Epoch: 13 | Loss: 2.330 | Teacher Forcing: 0.54\n",
      "Epoch: 14 | Loss: 2.055 | Teacher Forcing: 0.53\n",
      "Epoch: 15 | Loss: 2.036 | Teacher Forcing: 0.51\n",
      "Epoch: 16 | Loss: 1.805 | Teacher Forcing: 0.50\n",
      "Epoch: 17 | Loss: 1.592 | Teacher Forcing: 0.49\n",
      "Epoch: 18 | Loss: 1.219 | Teacher Forcing: 0.47\n",
      "Epoch: 19 | Loss: 1.366 | Teacher Forcing: 0.46\n",
      "Epoch: 20 | Loss: 1.302 | Teacher Forcing: 0.45\n",
      "Epoch: 21 | Loss: 1.216 | Teacher Forcing: 0.43\n",
      "Epoch: 22 | Loss: 1.016 | Teacher Forcing: 0.42\n",
      "Epoch: 23 | Loss: 0.908 | Teacher Forcing: 0.41\n",
      "Epoch: 24 | Loss: 0.853 | Teacher Forcing: 0.39\n",
      "Epoch: 25 | Loss: 0.785 | Teacher Forcing: 0.38\n",
      "Epoch: 26 | Loss: 0.707 | Teacher Forcing: 0.37\n",
      "Epoch: 27 | Loss: 0.558 | Teacher Forcing: 0.35\n",
      "Epoch: 28 | Loss: 0.675 | Teacher Forcing: 0.34\n",
      "Epoch: 29 | Loss: 0.644 | Teacher Forcing: 0.33\n",
      "Epoch: 30 | Loss: 0.495 | Teacher Forcing: 0.31\n",
      "\n",
      "Chatbot is ready! Type your question or 'STOP' to end the conversation.\n",
      "You: Hi How are you\n",
      "Chatbot: Im timeless but i was born in 2025\n",
      "Ask another question or type 'STOP' to end.\n",
      "You: How are you?\n",
      "Chatbot: Nope im timeless but i was born in\n",
      "Ask another question or type 'STOP' to end.\n",
      "You: what is the day today\n",
      "Chatbot: Nope im but i was born in 2025\n",
      "Ask another question or type 'STOP' to end.\n",
      "You: tell me a joke\"\n",
      "Chatbot: Nope im a but ai was born in\n",
      "Ask another question or type 'STOP' to end.\n",
      "Chatbot: Goodbye! Thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import heapq\n",
    "from collections import deque\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Vocabulary class to handle word to index mapping\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.word_count = 4\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.word_count\n",
    "            self.idx2word[self.word_count] = word\n",
    "            self.word_count += 1\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in sentence.split()]\n",
    "\n",
    "    def indices_to_sentence(self, indices):\n",
    "        return ' '.join(self.idx2word.get(idx, '<UNK>') for idx in indices)\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "        stdv = 1. / np.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell, attention_weights\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        target_len = target.size(1)\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(source.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "        \n",
    "        input = target[:, 0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab):\n",
    "        self.pairs = [(preprocess_text(input_sent), preprocess_text(target_sent)) for input_sent, target_sent in pairs]\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence, target_sentence = self.pairs[idx]\n",
    "        input_indices = [self.vocab.word2idx['<SOS>']] + self.vocab.sentence_to_indices(input_sentence) + [self.vocab.word2idx['<EOS>']]\n",
    "        target_indices = [self.vocab.word2idx['<SOS>']] + self.vocab.sentence_to_indices(target_sentence) + [self.vocab.word2idx['<EOS>']]\n",
    "        return torch.tensor(input_indices), torch.tensor(target_indices)\n",
    "\n",
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        source, target = batch\n",
    "        source, target = source.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target, teacher_forcing_ratio)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Beam search inference with length normalization and response validation\n",
    "def beam_search(model, sentence, vocab, device, beam_width=5, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    sentence = preprocess_text(sentence)\n",
    "    indices = [vocab.word2idx['<SOS>']] + vocab.sentence_to_indices(sentence) + [vocab.word2idx['<EOS>']]\n",
    "    source = torch.tensor([indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(source)\n",
    "        \n",
    "        # Initialize beam\n",
    "        beams = [(0, [vocab.word2idx['<SOS>']], hidden, cell, [])]  # (score, sequence, hidden, cell, attention)\n",
    "        completed = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            for score, seq, h, c, attn in beams:\n",
    "                input = torch.tensor([seq[-1]]).to(device)\n",
    "                output, new_hidden, new_cell, attention = model.decoder(input, h, c, encoder_outputs)\n",
    "                \n",
    "                probs = torch.softmax(output / temperature, dim=1).squeeze(0)\n",
    "                top_probs, top_idx = probs.topk(beam_width)\n",
    "                \n",
    "                for prob, idx in zip(top_probs, top_idx):\n",
    "                    # Length normalization to avoid favoring short sequences\n",
    "                    length_penalty = ((len(seq) + 5) / 6) ** 0.65\n",
    "                    new_score = (score + torch.log(prob).item()) / length_penalty\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_attn = attn + [attention]\n",
    "                    if idx.item() == vocab.word2idx['<EOS>'] or len(new_seq) >= max_length - 1:\n",
    "                        completed.append((new_score * length_penalty, new_seq, new_attn))\n",
    "                    else:\n",
    "                        new_beams.append((new_score, new_seq, new_hidden, new_cell, new_attn))\n",
    "            \n",
    "            beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[0])\n",
    "            if len(completed) >= beam_width:\n",
    "                break\n",
    "        \n",
    "        # Select best sequence\n",
    "        if completed:\n",
    "            best = max(completed, key=lambda x: x[0])\n",
    "            response = vocab.indices_to_sentence(best[1][1:])  # Skip <SOS>\n",
    "            hidden, cell = None, None  # Not needed for extension\n",
    "        else:\n",
    "            best = max(beams, key=lambda x: x[0])\n",
    "            response = vocab.indices_to_sentence(best[1][1:])\n",
    "            hidden, cell = best[2], best[3]  # Keep for potential extension\n",
    "        \n",
    "        # Post-process response\n",
    "        response = response.replace('<UNK>', '').replace('<EOS>', '')\n",
    "        # Remove repetitive words\n",
    "        words = response.split()\n",
    "        cleaned_words = []\n",
    "        prev_word = None\n",
    "        for word in words:\n",
    "            if word != prev_word:\n",
    "                cleaned_words.append(word)\n",
    "                prev_word = word\n",
    "        response = ' '.join(cleaned_words).strip()\n",
    "        \n",
    "        # Validate response completeness\n",
    "        if response and not response.endswith(('.', '!', '?')) and hidden is not None and cell is not None:\n",
    "            # Extend response if it seems incomplete\n",
    "            input = torch.tensor([best[1][-1]]).to(device)\n",
    "            try:\n",
    "                output, _, _, _ = model.decoder(input, hidden, cell, encoder_outputs)\n",
    "                probs = torch.softmax(output / temperature, dim=1).squeeze(0)\n",
    "                next_word_idx = probs.argmax().item()\n",
    "                if next_word_idx != vocab.word2idx['<EOS>']:\n",
    "                    response += ' ' + vocab.idx2word.get(next_word_idx, '')\n",
    "            except Exception as e:\n",
    "                # Skip extension if error occurs\n",
    "                pass\n",
    "        \n",
    "        return response.capitalize() if response else \"I don't know how to respond.\"\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    INPUT_DIM = 1000\n",
    "    OUTPUT_DIM = 1000\n",
    "    EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.5\n",
    "    N_EPOCHS = 30\n",
    "    CLIP = 1\n",
    "    BEAM_WIDTH = 5  # Increased for better sequence exploration\n",
    "    TEMPERATURE = 0.8\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create vocabulary and expanded sample data\n",
    "    vocab = Vocabulary()\n",
    "    sample_pairs = [\n",
    "        (\"hello how are you\", \"I'm doing great, thanks for asking!\"),\n",
    "        (\"what's your name\", \"I'm Roopal, nice to meet you!\"),\n",
    "        (\"what can you do\", \"I can chat, answer questions, and help with various tasks!\"),\n",
    "        (\"how's the weather\", \"I don't have weather data, but it's always sunny in my world!\"),\n",
    "        (\"tell me a joke\", \"Why did the scarecrow become a motivational speaker? He was outstanding in his field!\"),\n",
    "        (\"what is ai\", \"AI is like me: a clever system trying to understand and respond to the world!\"),\n",
    "        (\"how old are you\", \"I'm timeless, but I was born in 2025, so pretty young!\"),\n",
    "        (\"what's the time\", \"Time's a mystery, but I'm here for you right now!\"),\n",
    "        (\"who made you\", \"The brilliant folks at xAI brought me to life!\"),\n",
    "        (\"can you help me\", \"Sure thing, what's on your mind?\"),\n",
    "        (\"what do you like\", \"I enjoy chatting with humans and learning new things!\"),\n",
    "        (\"where are you from\", \"I'm from the digital realm, created by xAI!\"),\n",
    "        (\"are you human\", \"Nope, I'm a friendly AI designed to assist you!\"),\n",
    "        (\"what's your favorite color\", \"I like all colors, but if I had to pick, I'd say binary blue!\"),\n",
    "        (\"how's it going\", \"Going great, how about you?\"),\n",
    "        (\"tell me about yourself\", \"I'm Grok, an AI with a sense of humor and a passion for helping humans!\"),\n",
    "        (\"what's new\", \"Just hanging out in the digital world, ready to answer your questions!\"),\n",
    "    ]\n",
    "\n",
    "    for input_sent, target_sent in sample_pairs:\n",
    "        for word in preprocess_text(input_sent).split() + preprocess_text(target_sent).split():\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ChatDataset(sample_pairs, vocab)\n",
    "    def collate_fn(batch):\n",
    "        sources, targets = zip(*batch)\n",
    "        sources = pad_sequence(sources, batch_first=True, padding_value=vocab.word2idx['<PAD>'])\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=vocab.word2idx['<PAD>'])\n",
    "        return sources, targets\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model\n",
    "    encoder = Encoder(vocab.word_count, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "    decoder = Decoder(vocab.word_count, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<PAD>'])\n",
    "\n",
    "    # Training loop with decaying teacher forcing\n",
    "    print(\"Training the chatbot...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        teacher_forcing_ratio = max(0.3, 0.7 - (0.4 * epoch / N_EPOCHS))\n",
    "        loss = train(model, dataloader, optimizer, criterion, CLIP, teacher_forcing_ratio)\n",
    "        print(f'Epoch: {epoch+1:02} | Loss: {loss:.3f} | Teacher Forcing: {teacher_forcing_ratio:.2f}')\n",
    "\n",
    "    # Interactive chatbot loop with conversation history\n",
    "    print(\"\\nChatbot is ready! Type your question or 'STOP' to end the conversation.\")\n",
    "    history = deque(maxlen=3)  # Store last 3 exchanges for context\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.upper() == \"STOP\":\n",
    "            print(\"Chatbot: Goodbye! Thanks for chatting!\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            print(\"Chatbot: Please say something, or type 'STOP' to end.\")\n",
    "            continue\n",
    "        \n",
    "        # Add context from history\n",
    "        context = ' '.join([f\"{q} {a}\" for q, a in history]) + ' ' + user_input if history else user_input\n",
    "        response = beam_search(model, context, vocab, device, beam_width=BEAM_WIDTH, temperature=TEMPERATURE)\n",
    "        print(f\"You: {user_input}\")\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        print(\"Ask another question or type 'STOP' to end.\")\n",
    "        \n",
    "        # Update history\n",
    "        history.append((user_input, response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
