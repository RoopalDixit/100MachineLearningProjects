{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c769a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Define constants\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "DATA_DIR = 'PetImages'\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Prepare training and validation data from PetImages directory structure\n",
    "    \"\"\"\n",
    "    # Create data generators with validation split\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2  # Use 20% for validation\n",
    "    )\n",
    "\n",
    "    # Load training data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        subset='training',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Load validation data (no augmentation)\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        subset='validation',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {train_generator.samples}\")\n",
    "    print(f\"Validation samples: {validation_generator.samples}\")\n",
    "    print(f\"Class indices: {train_generator.class_indices}\")\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Build transfer learning model\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build a transfer learning model using VGG16 as base\n",
    "    \"\"\"\n",
    "    # Load pre-trained VGG16 model without top layers\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "\n",
    "    # Freeze the convolutional base initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create new model with GlobalAveragePooling2D for better performance\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy/loss\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, validation_generator):\n",
    "    \"\"\"\n",
    "    Evaluate model and show detailed metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    validation_generator.reset()\n",
    "    predictions = model.predict(validation_generator, verbose=1)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Get true labels\n",
    "    true_classes = validation_generator.classes\n",
    "    class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return predictions, predicted_classes\n",
    "\n",
    "def test_model_on_samples(model_path='cats_vs_dogs_transfer_learning_model.h5', num_samples=6):\n",
    "    \"\"\"\n",
    "    Test the trained model on random sample images\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    except:\n",
    "        print(\"Model not found. Please train the model first.\")\n",
    "        return\n",
    "\n",
    "    # Get random sample images\n",
    "    cat_dir = os.path.join(DATA_DIR, 'Cat')\n",
    "    dog_dir = os.path.join(DATA_DIR, 'Dog')\n",
    "\n",
    "    cat_images = [f for f in os.listdir(cat_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    dog_images = [f for f in os.listdir(dog_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Select random samples\n",
    "    sample_cats = random.sample(cat_images, min(num_samples//2, len(cat_images)))\n",
    "    sample_dogs = random.sample(dog_images, min(num_samples//2, len(dog_images)))\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_samples//2, figsize=(15, 8))\n",
    "\n",
    "    # Test cat images\n",
    "    for i, img_name in enumerate(sample_cats):\n",
    "        img_path = os.path.join(cat_dir, img_name)\n",
    "\n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array /= 255.0\n",
    "\n",
    "            # Make prediction\n",
    "            prediction = model.predict(img_array)[0][0]\n",
    "            predicted_class = \"Dog\" if prediction > 0.5 else \"Cat\"\n",
    "            confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "\n",
    "            # Display image\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title(f'True: Cat\\nPred: {predicted_class}\\nConf: {confidence:.3f}')\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            axes[0, i].text(0.5, 0.5, 'Image Error', ha='center', va='center')\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "    # Test dog images\n",
    "    for i, img_name in enumerate(sample_dogs):\n",
    "        img_path = os.path.join(dog_dir, img_name)\n",
    "\n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array /= 255.0\n",
    "\n",
    "            # Make prediction\n",
    "            prediction = model.predict(img_array)[0][0]\n",
    "            predicted_class = \"Dog\" if prediction > 0.5 else \"Cat\"\n",
    "            confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "\n",
    "            # Display image\n",
    "            axes[1, i].imshow(img)\n",
    "            axes[1, i].set_title(f'True: Dog\\nPred: {predicted_class}\\nConf: {confidence:.3f}')\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            axes[1, i].text(0.5, 0.5, 'Image Error', ha='center', va='center')\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    train_generator, validation_generator = prepare_data()\n",
    "\n",
    "    # Build model\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model, base_model = build_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.0001)\n",
    "    ]\n",
    "\n",
    "    # Train model (initial training with frozen base)\n",
    "    print(\"\\nTraining model (frozen base)...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fine-tuning: unfreeze some layers of the base model\n",
    "    print(\"\\nFine-tuning model...\")\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 100\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Use a lower learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001/10),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Continue training for fine-tuning\n",
    "    fine_tune_epochs = 10\n",
    "    total_epochs = len(history.history['loss']) + fine_tune_epochs\n",
    "\n",
    "    history_fine = model.fit(\n",
    "        train_generator,\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=len(history.history['loss']),\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Combine histories\n",
    "    for key in history.history:\n",
    "        history.history[key].extend(history_fine.history[key])\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    val_loss, val_accuracy = model.evaluate(validation_generator, verbose=1)\n",
    "    print(f\"Final Validation accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Final Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Detailed evaluation\n",
    "    predictions, predicted_classes = evaluate_model(model, validation_generator)\n",
    "\n",
    "    # Save model\n",
    "    model.save('cats_vs_dogs_transfer_learning_model.h5')\n",
    "    print(\"\\nModel saved as 'cats_vs_dogs_transfer_learning_model.h5'\")\n",
    "\n",
    "    # Test on sample images\n",
    "    print(\"\\nTesting on sample images...\")\n",
    "    test_model_on_samples()\n",
    "\n",
    "    return history, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
